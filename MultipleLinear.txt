import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

url = "student-mat.csv"

X = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob',
          'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup',
          'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime',
          'goout', 'Dalc', 'Walc', 'health', 'absences']]
y = data['G3']

# Convert categorical variables to numerical using one-hot encoding
X = pd.get_dummies(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared Value: {r2}')

plt.scatter(y_test, y_pred)
plt.xlabel("Actual Final Grade (y_test)")
plt.ylabel("Predicted Final Grade (y_pred)")
plt.title("Actual vs Predicted Final Grade")

plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', label='Regression Line')

plt.legend()
plt.show()

intercept = model.intercept_
coefficients = model.coef_

equation = f'y = {intercept:.2f} + '
equation += ' + '.join([f'{coeff:.2f} * {feature}' for coeff, feature in zip(coefficients, X.columns)])

print("Equation of the Regression Line:")
print(equation)
